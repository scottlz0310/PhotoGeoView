#!/usr/bin/env python3
"""
AIçµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«

CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµæœã‚’çµ±åˆã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

AIè²¢çŒ®è€…:
- Kiro: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»å®Ÿè£…

ä½œæˆè€…: Kiro AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
ä½œæˆæ—¥: 2025å¹´1æœˆ26æ—¥
"""

import json
import subprocess
import sys
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional


@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœ"""

    name: str
    status: str  # passed, failed, skipped
    duration: float
    error_message: Optional[str] = None


@dataclass
class QualityMetrics:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    total_files: int
    total_issues: int
    coverage_percentage: float
    quality_score: float
    issues_by_severity: Dict[str, int]


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    total_tests: int
    regressions: int
    improvements: int
    average_execution_time: float
    memory_usage_mb: float


@dataclass
class IntegrationReport:
    """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ"""

    timestamp: datetime
    build_status: str
    test_results: List[TestResult]
    quality_metrics: QualityMetrics
    performance_metrics: PerformanceMetrics
    ai_component_status: Dict[str, str]
    recommendations: List[str]


class IntegrationReportGenerator:
    """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.reports_dir = project_root / "reports"
        self.reports_dir.mkdir(exist_ok=True)

    def collect_test_results(self) -> List[TestResult]:
        """ãƒ†ã‚¹ãƒˆçµæœã‚’åé›†"""
        test_results = []

        # pytestçµæœã®åé›†
        junit_files = list(self.project_root.glob("**/junit*.xml"))
        junit_files.extend(list(self.project_root.glob("**/test-results*.xml")))

        for junit_file in junit_files:
            try:
                tree = ET.parse(junit_file)
                root = tree.getroot()

                for testcase in root.findall(".//testcase"):
                    name = testcase.get("name", "unknown")
                    duration = float(testcase.get("time", 0))

                    failure = testcase.find("failure")
                    error = testcase.find("error")
                    skipped = testcase.find("skipped")

                    if failure is not None:
                        status = "failed"
                        error_message = failure.text
                    elif error is not None:
                        status = "failed"
                        error_message = error.text
                    elif skipped is not None:
                        status = "skipped"
                        error_message = skipped.text
                    else:
                        status = "passed"
                        error_message = None

                    test_results.append(
                        TestResult(
                            name=name,
                            status=status,
                            duration=duration,
                            error_message=error_message,
                        )
                    )

            except Exception as e:
                print(f"JUnit XMLãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {junit_file}: {e}")

        return test_results

    def collect_quality_metrics(self) -> QualityMetrics:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        # AIå“è³ªãƒã‚§ãƒƒã‚«ãƒ¼ã®çµæœã‚’åé›†
        quality_report_path = self.project_root / "quality_report.json"

        if quality_report_path.exists():
            try:
                with open(quality_report_path, "r", encoding="utf-8") as f:
                    quality_data = json.load(f)

                return QualityMetrics(
                    total_files=quality_data.get("total_files", 0),
                    total_issues=quality_data.get("total_issues", 0),
                    coverage_percentage=quality_data.get("coverage_percentage", 0.0),
                    quality_score=quality_data.get("overall_score", 0.0),
                    issues_by_severity=quality_data.get("issues_by_severity", {}),
                )
            except Exception as e:
                print(f"å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        return QualityMetrics(
            total_files=0,
            total_issues=0,
            coverage_percentage=0.0,
            quality_score=0.0,
            issues_by_severity={},
        )

    def collect_performance_metrics(self) -> PerformanceMetrics:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®çµæœã‚’åé›†
        benchmark_files = list(self.project_root.glob("**/benchmark*.json"))

        total_tests = 0
        regressions = 0
        improvements = 0
        execution_times = []
        memory_usages = []

        for benchmark_file in benchmark_files:
            try:
                with open(benchmark_file, "r", encoding="utf-8") as f:
                    benchmark_data = json.load(f)

                benchmarks = benchmark_data.get("benchmarks", [])
                total_tests += len(benchmarks)

                for benchmark in benchmarks:
                    stats = benchmark.get("stats", {})
                    mean_time = stats.get("mean", 0)
                    execution_times.append(mean_time)

                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆä»®æƒ³å€¤ï¼‰
                    memory_usages.append(mean_time * 10)  # ä»®ã®è¨ˆç®—

            except Exception as e:
                print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {benchmark_file}: {e}")

        return PerformanceMetrics(
            total_tests=total_tests,
            regressions=regressions,
            improvements=improvements,
            average_execution_time=(
                sum(execution_times) / len(execution_times) if execution_times else 0.0
            ),
            memory_usage_mb=(
                sum(memory_usages) / len(memory_usages) if memory_usages else 0.0
            ),
        )

    def check_ai_component_status(self) -> Dict[str, str]:
        """AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        status = {}

        # å„AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆçµæœã‚’ç¢ºèª
        ai_components = ["copilot", "cursor", "kiro"]

        for component in ai_components:
            test_dir = self.project_root / f"tests/{component}_tests"

            if test_dir.exists():
                # ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                try:
                    result = subprocess.run(
                        [
                            sys.executable,
                            "-m",
                            "pytest",
                            str(test_dir),
                            "--tb=no",
                            "-q",
                        ],
                        capture_output=True,
                        text=True,
                        cwd=self.project_root,
                    )

                    if result.returncode == 0:
                        status[component] = "âœ… æ­£å¸¸"
                    else:
                        status[component] = "âŒ å¤±æ•—"

                except Exception as e:
                    status[component] = f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}"
            else:
                status[component] = "âš ï¸ ãƒ†ã‚¹ãƒˆæœªå®Ÿè£…"

        return status

    def generate_recommendations(self, report: IntegrationReport) -> List[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        failed_tests = [t for t in report.test_results if t.status == "failed"]
        if failed_tests:
            recommendations.append(
                f"âŒ {len(failed_tests)}å€‹ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’ä¿®æ­£ã—ã¦ãã ã•ã„"
            )

        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if report.quality_metrics.quality_score < 70:
            recommendations.append(
                f"âš ï¸ å“è³ªã‚¹ã‚³ã‚¢({report.quality_metrics.quality_score:.1f})ãŒä½ã„ãŸã‚ã€ã‚³ãƒ¼ãƒ‰å“è³ªã®æ”¹å–„ãŒå¿…è¦ã§ã™"
            )

        if report.quality_metrics.coverage_percentage < 80:
            recommendations.append(
                f"ğŸ“Š ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸({report.quality_metrics.coverage_percentage:.1f}%)ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if report.performance_metrics.regressions > 0:
            recommendations.append(
                f"ğŸŒ {report.performance_metrics.regressions}ä»¶ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„"
            )

        # AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçŠ¶æ…‹ã«åŸºã¥ãæ¨å¥¨äº‹é …
        failed_components = [
            comp
            for comp, status in report.ai_component_status.items()
            if "âŒ" in status
        ]
        if failed_components:
            recommendations.append(
                f"ğŸ¤– AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ({', '.join(failed_components)})ã®å•é¡Œã‚’è§£æ±ºã—ã¦ãã ã•ã„"
            )

        if not recommendations:
            recommendations.append("âœ… ã™ã¹ã¦ã®å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™")

        return recommendations

    def generate_report(self) -> IntegrationReport:
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ãƒ†ã‚¹ãƒˆçµæœã‚’åé›†ä¸­...")
        test_results = self.collect_test_results()

        print("å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ä¸­...")
        quality_metrics = self.collect_quality_metrics()

        print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ä¸­...")
        performance_metrics = self.collect_performance_metrics()

        print("AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
        ai_component_status = self.check_ai_component_status()

        # ãƒ“ãƒ«ãƒ‰çŠ¶æ…‹ã‚’åˆ¤å®š
        failed_tests = [t for t in test_results if t.status == "failed"]
        build_status = "âŒ å¤±æ•—" if failed_tests else "âœ… æˆåŠŸ"

        report = IntegrationReport(
            timestamp=datetime.now(),
            build_status=build_status,
            test_results=test_results,
            quality_metrics=quality_metrics,
            performance_metrics=performance_metrics,
            ai_component_status=ai_component_status,
            recommendations=[],
        )

        # æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
        report.recommendations = self.generate_recommendations(report)

        return report

    def save_report_markdown(
        self, report: IntegrationReport, output_path: Path
    ) -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        lines = [
            "# PhotoGeoView AIçµ±åˆ CI/CDãƒ¬ãƒãƒ¼ãƒˆ",
            "",
            f"**ç”Ÿæˆæ—¥æ™‚**: {report.timestamp.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
            f"**ãƒ“ãƒ«ãƒ‰çŠ¶æ…‹**: {report.build_status}",
            "",
            "## ğŸ“Š æ¦‚è¦",
            "",
            f"- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {len(report.test_results)}",
            f"- **æˆåŠŸ**: {len([t for t in report.test_results if t.status == 'passed'])}",
            f"- **å¤±æ•—**: {len([t for t in report.test_results if t.status == 'failed'])}",
            f"- **ã‚¹ã‚­ãƒƒãƒ—**: {len([t for t in report.test_results if t.status == 'skipped'])}",
            f"- **å“è³ªã‚¹ã‚³ã‚¢**: {report.quality_metrics.quality_score:.1f}/100",
            f"- **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: {report.quality_metrics.coverage_percentage:.1f}%",
            "",
            "## ğŸ¤– AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçŠ¶æ…‹",
            "",
        ]

        for component, status in report.ai_component_status.items():
            lines.append(f"- **{component.upper()}**: {status}")

        lines.extend(["", "## ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœè©³ç´°", ""])

        # å¤±æ•—ãƒ†ã‚¹ãƒˆã®è©³ç´°
        failed_tests = [t for t in report.test_results if t.status == "failed"]
        if failed_tests:
            lines.extend(["### âŒ å¤±æ•—ãƒ†ã‚¹ãƒˆ", ""])

            for test in failed_tests:
                lines.extend(
                    [
                        f"#### {test.name}",
                        f"- **å®Ÿè¡Œæ™‚é–“**: {test.duration:.3f}ç§’",
                        f"- **ã‚¨ãƒ©ãƒ¼**: {test.error_message or 'ã‚¨ãƒ©ãƒ¼è©³ç´°ãªã—'}",
                        "",
                    ]
                )

        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°
        lines.extend(
            [
                "## ğŸ“ˆ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                "",
                f"- **ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {report.quality_metrics.total_files}",
                f"- **ç·å•é¡Œæ•°**: {report.quality_metrics.total_issues}",
                f"- **å“è³ªã‚¹ã‚³ã‚¢**: {report.quality_metrics.quality_score:.1f}/100",
                "",
            ]
        )

        if report.quality_metrics.issues_by_severity:
            lines.extend(["### é‡è¦åº¦åˆ¥å•é¡Œæ•°", ""])

            for severity, count in report.quality_metrics.issues_by_severity.items():
                if count > 0:
                    lines.append(f"- **{severity}**: {count}ä»¶")

            lines.append("")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        lines.extend(
            [
                "## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                "",
                f"- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæ•°**: {report.performance_metrics.total_tests}",
                f"- **å›å¸°æ•°**: {report.performance_metrics.regressions}",
                f"- **æ”¹å–„æ•°**: {report.performance_metrics.improvements}",
                f"- **å¹³å‡å®Ÿè¡Œæ™‚é–“**: {report.performance_metrics.average_execution_time:.4f}ç§’",
                f"- **å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {report.performance_metrics.memory_usage_mb:.2f}MB",
                "",
                "## ğŸ’¡ æ¨å¥¨äº‹é …",
                "",
            ]
        )

        for recommendation in report.recommendations:
            lines.append(f"- {recommendation}")

        lines.extend(["", "---", "*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*"])

        output_path.write_text("\n".join(lines), encoding="utf-8")

    def save_report_json(self, report: IntegrationReport, output_path: Path) -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜"""
        report_data = {
            "timestamp": report.timestamp.isoformat(),
            "build_status": report.build_status,
            "test_results": [
                {
                    "name": t.name,
                    "status": t.status,
                    "duration": t.duration,
                    "error_message": t.error_message,
                }
                for t in report.test_results
            ],
            "quality_metrics": {
                "total_files": report.quality_metrics.total_files,
                "total_issues": report.quality_metrics.total_issues,
                "coverage_percentage": report.quality_metrics.coverage_percentage,
                "quality_score": report.quality_metrics.quality_score,
                "issues_by_severity": report.quality_metrics.issues_by_severity,
            },
            "performance_metrics": {
                "total_tests": report.performance_metrics.total_tests,
                "regressions": report.performance_metrics.regressions,
                "improvements": report.performance_metrics.improvements,
                "average_execution_time": report.performance_metrics.average_execution_time,
                "memory_usage_mb": report.performance_metrics.memory_usage_mb,
            },
            "ai_component_status": report.ai_component_status,
            "recommendations": report.recommendations,
        }

        output_path.write_text(
            json.dumps(report_data, ensure_ascii=False, indent=2), encoding="utf-8"
        )


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="AIçµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    parser.add_argument(
        "--output-dir", type=Path, default=Path("reports"), help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--format",
        choices=["markdown", "json", "both"],
        default="both",
        help="å‡ºåŠ›å½¢å¼",
    )

    args = parser.parse_args()

    project_root = Path(__file__).parent.parent
    generator = IntegrationReportGenerator(project_root)

    print("AIçµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    report = generator.generate_report()

    args.output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = report.timestamp.strftime("%Y%m%d_%H%M%S")

    if args.format in ["markdown", "both"]:
        markdown_path = args.output_dir / f"integration_report_{timestamp}.md"
        generator.save_report_markdown(report, markdown_path)
        print(f"âœ… Markdownãƒ¬ãƒãƒ¼ãƒˆ: {markdown_path}")

    if args.format in ["json", "both"]:
        json_path = args.output_dir / f"integration_report_{timestamp}.json"
        generator.save_report_json(report, json_path)
        print(f"âœ… JSONãƒ¬ãƒãƒ¼ãƒˆ: {json_path}")

    # æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆ
    if args.format in ["markdown", "both"]:
        latest_md = args.output_dir / "latest_integration_report.md"
        if latest_md.exists():
            latest_md.unlink()
        try:
            latest_md.symlink_to(markdown_path.name)
        except OSError:
            # Windowsã§ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ãŒä½œæˆã§ããªã„å ´åˆã¯ã‚³ãƒ”ãƒ¼
            import shutil

            shutil.copy2(markdown_path, latest_md)

    print(f"\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆæ¦‚è¦:")
    print(f"  ãƒ“ãƒ«ãƒ‰çŠ¶æ…‹: {report.build_status}")
    print(
        f"  ãƒ†ã‚¹ãƒˆçµæœ: {len([t for t in report.test_results if t.status == 'passed'])}/{len(report.test_results)} æˆåŠŸ"
    )
    print(f"  å“è³ªã‚¹ã‚³ã‚¢: {report.quality_metrics.quality_score:.1f}/100")

    # å¤±æ•—ãŒã‚ã‚‹å ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1
    failed_tests = [t for t in report.test_results if t.status == "failed"]
    if failed_tests or report.quality_metrics.quality_score < 70:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
